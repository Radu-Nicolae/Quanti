\documentclass[sigconf,review]{acmart}
\usepackage{amssymb}
\usepackage{pifont} % for \cmark and \xmark
\usepackage{pifont} % for \cmark and \xmark
\newcommand{\cmark}{\ding{51}} % check mark
\newcommand{\xmark}{\ding{55}} % cross mark

\usepackage{amsmath,amssymb,amsfonts,latexsym}
\usepackage{enumerate}
\usepackage{lipsum}
\usepackage{xspace}
\usepackage{epsf,picinpar}
\usepackage{hyperref}
\usepackage{varioref}
\usepackage{varioref}
\usepackage{tabularx}
\usepackage{colortbl,multirow,hhline}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{colortbl,multirow,hhline}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{xcolor,colortbl}
\usepackage{url}
\usepackage{balance}
\usepackage{graphicx, subfigure}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{listings}
\usepackage{framed}
\usepackage{morefloats}
\usepackage[T1]{fontenc}
\usepackage{array}
\usepackage{pdfpages}
\usepackage{fancybox}
\usepackage{amsmath}
\usepackage{flushend}
\usepackage{booktabs}
\usepackage{enumitem}

\renewcommand{\ttdefault}{cmr}

%\newcommand{\limit}[1]{\textcolor{red}{\noindent \ding{46}~Page limit:~#1}\\}
\newcommand{\todo}[1]{\textcolor{blue}{\ding{46}~#1}} 
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\etal}{\emph{et~al.}\xspace} 

\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmcopyright}
\acmConference[Green Lab 2025]{Green Lab 2025/2026 - Vrije Universiteit Amsterdam}
\acmBooktitle{Vrije Universiteit Amsterdam}
\acmPrice{}
\acmDOI{}
\acmISBN{}

    
\begin{document}


\title{
Exploring the impact of LLM compression by quantization on energy consumption, resource utilisation, and accuracy
}

\author{Radu Nicolae}
\affiliation{%
 \institution{2760443 \\ VU Amsterdam, \\ Universiteit van Amsterdam}
} \email{r.nicolae@vu.nl}

\author{Junaid Khalil}
\affiliation{%
 \institution{2891638 \\ VU Amsterdam, \\
 Universiteit van Amsterdam}
} \email{j.khalil6@student.vu.nl}

\author{Muhammad Shayan}
\affiliation{%
 \institution{2891482 \\ VU Amsterdam}
} \email{m.shayan@student.vu.nl}

\author{Md Tasluf Morshed}
\affiliation{%
 \institution{2890836 \\ VU Amsterdam}
} \email{m.t.morshed@student.vu.nl}

\author{Alp Eren Inceoglu}
\affiliation{%
 \institution{2842116 \\ VU Amsterdam}
} \email{a.e.inceoglu@student.vu.nl}

\begin{abstract}
Large Language Models (LLMs) are being increasingly adopted by our digital society, but raise sustainability concerns when operated at a massive societal scale. % societal context
To improve the performance, efficiency, and, thus, sustainability of LLMs, compression techniques have been widely adopted, yet at the cost of accuracy. % LLM context
One such technique is quantization, a state-of-the-art approach to reduce model size (thus computation requirements), while maintaining sufficient accuracy, and without architectural changes or re-training. % quantization context
However, although quantization is theoretically expected to generate tradeoffs between energy efficiency, resource utilization (e.g., CPU, memory, inference time), and accuracy degradation, these tradeoffs are insufficiently understood and unquantified. % gap 1
The quantification challenge is further exacerbated by the absence of benchmarking systems for systematically quantifying LLM ecosystems; the lack of such systems can be costly and could misguide operators of LLM services. % gap 2
Addressing the open challenge, in this work, we propose \underline{Quanti}, the first tool for \underline{quanti}fying the sustainability and performance of LLMs. % approach (following AtLarge, community-standard methodology on designing distributed (eco)systems)
We propose an architecture for deploying, measuring, and comparing LLM on the server side. % design
Through experiments with a prototype, % implement
we % evaluate & experiment
(i) explore the impact of quantization on operational-level metrics of LLMs,
(ii) evaluate the tradeoff between energy consumption-accuracy of LLMs underlying different architectures, both original and under compression by quantization, and
(iii) quantify the impact of CO2-aware workload scheduling of compressed and uncompressed models. 
Quanti, together with all the production data, results, and reproducibility capsule, are released open-source on \url{https://github.com/Radu-Nicolae/Quanti}.

\vspace*{-0.1cm}



% \noindent \textit{Goal}. 
% \todo{at the end}

% \noindent \textit{Method}. 
% \todo{at the end}

% \noindent \textit{Results}. 
% \todo{at the end}

% \noindent \textit{Conclusions}. 
% \todo{at the end}
\end{abstract}


\maketitle

\input{reportTemplate/sections/intro} \newpage
\input{reportTemplate/sections/background}
\input{reportTemplate/sections/related}
\input{reportTemplate/sections/definition}
\input{reportTemplate/sections/planning}
\input{reportTemplate/sections/design}
\input{reportTemplate/sections/execution}
\input{reportTemplate/sections/results}
\input{reportTemplate/sections/discussion}
\input{reportTemplate/sections/threats}
\input{reportTemplate/sections/conclusion} 

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{reportTemplate/figures/paper-page-budgeting-green-lab.pdf}
    \caption{Page Budgeting following the template suggested in the work in progress paper by Iosup et al. in \cite{iosup-epema-systems-writing-wip}. Edit link: \url{https://drive.google.com/file/d/1-9jcqbOQJRCJZpWZldTrm27HNUFuAtWR/view?usp=drive_link}.}
    \label{fig:placeholder}
\end{figure*}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
